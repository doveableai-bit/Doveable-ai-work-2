अब मैं **powerful LLM service** बनाता हूँ:
Action: {"tool_name": "mcp_bulk_file_writer", "tool_arguments": {"files": [{"path": "/app/backend/.env", "content": "MONGO_URL=\"mongodb://localhost:27017\"\nDB_NAME=\"doveable_ai\"\nCORS_ORIGINS=\"*\"\n\n# AI/LLM Configuration\n# Emergent LLM (Default - Works with OpenAI, Claude, Gemini)\nEMERGENT_LLM_KEY=\"sk-emergent-052E0B569A286C0921\"\n\n# Groq AI API Key\nGROQ_API_KEY=\"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\"\n\n# Google Gemini API Key (Direct)\nGOOGLE_GEMINI_API_KEY=\"AIzaSyCH2H9FHoqQaCcmu43ttpkrUsaxi1wI3hw\"\n\n# JWT Authentication\nJWT_SECRET=\"doveable-ai-secret-key-2024\"\nJWT_ALGORITHM=\"HS256\"\nJWT_EXPIRATION_HOURS=720\n\n# User Custom Backend (Optional)\nALLOW_CUSTOM_BACKEND=true"}, {"path": "/app/backend/services/__init__.py", "content": "# Services module"}, {"path": "/app/backend/services/llm_service.py", "content": "import os\nimport logging\nfrom typing import Optional, Dict, Any\nfrom groq import Groq\nimport google.generativeai as genai\nfrom emergentintegrations.llm.chat import LlmChat, UserMessage\n\nlogger = logging.getLogger(__name__)\n\nclass DoveableAIService:\n    \"\"\"\n    Doveable AI's Powerful Multi-LLM Service\n    \n    Supports:\n    1. Emergent LLM (Default) - OpenAI GPT-4o, Claude, Gemini via Universal Key\n    2. Groq AI - Lightning fast inference (Llama, Mixtral)\n    3. Google Gemini - Direct API integration\n    \n    Automatic fallback: Emergent \u2192 Groq \u2192 Gemini\n    \"\"\"\n    \n    def __init__(self):\n        # Load API keys\n        self.emergent_key = os.environ.get('EMERGENT_LLM_KEY')\n        self.groq_key = os.environ.get('GROQ_API_KEY')\n        self.gemini_key = os.environ.get('GOOGLE_GEMINI_API_KEY')\n        \n        # Initialize clients\n        self.groq_client = None\n        if self.groq_key:\n            try:\n                self.groq_client = Groq(api_key=self.groq_key)\n                logger.info(\"\u2705 Groq AI client initialized\")\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Groq initialization failed: {e}\")\n        \n        if self.gemini_key:\n            try:\n                genai.configure(api_key=self.gemini_key)\n                logger.info(\"\u2705 Google Gemini client initialized\")\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Gemini initialization failed: {e}\")\n        \n        logger.info(\"\ud83d\ude80 Doveable AI Service initialized with multi-LLM support\")\n    \n    async def generate_code(self, prompt: str, session_id: str, chat_history: list = None) -> Dict[str, Any]:\n        \"\"\"\n        Generate website code using AI with automatic fallback\n        \n        Returns:\n        {\n            'response': str,  # AI response text\n            'code': str,      # Extracted HTML/CSS/JS code\n            'provider': str   # Which AI provider was used\n        }\n        \"\"\"\n        \n        system_message = \"\"\"You are Doveable AI, an expert full-stack web developer. \n        \nYour job is to create beautiful, modern, and fully functional websites based on user descriptions.\n        \nIMPORTANT RULES:\n1. Always generate COMPLETE, ready-to-use HTML files\n2. Include ALL CSS inline within <style> tags\n3. Include ALL JavaScript inline within <script> tags\n4. Make designs modern, responsive, and beautiful\n5. Use modern CSS (flexbox, grid, gradients, animations)\n6. Ensure mobile responsiveness\n7. Add interactive elements when appropriate\n8. Use semantic HTML5\n9. Keep code clean and well-commented\n\nFormat your response with the HTML code in a code block:\n```html\n[YOUR COMPLETE HTML CODE HERE]\n```\n\nCreate amazing websites that users will love!\"\"\"\n        \n        # Strategy 1: Try Emergent LLM (Default - Best for code generation)\n        if self.emergent_key:\n            try:\n                logger.info(\"\ud83c\udfaf Trying Emergent LLM (OpenAI GPT-4o)...\")\n                result = await self._try_emergent_llm(prompt, session_id, system_message)\n                if result:\n                    logger.info(\"\u2705 Emergent LLM success!\")\n                    return result\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Emergent LLM failed: {e}\")\n        \n        # Strategy 2: Try Groq AI (Fastest)\n        if self.groq_client:\n            try:\n                logger.info(\"\ud83c\udfaf Trying Groq AI (Llama 3.1)...\")\n                result = await self._try_groq(prompt, system_message)\n                if result:\n                    logger.info(\"\u2705 Groq AI success!\")\n                    return result\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Groq AI failed: {e}\")\n        \n        # Strategy 3: Try Google Gemini (Backup)\n        if self.gemini_key:\n            try:\n                logger.info(\"\ud83c\udfaf Trying Google Gemini...\")\n                result = await self._try_gemini(prompt, system_message)\n                if result:\n                    logger.info(\"\u2705 Google Gemini success!\")\n                    return result\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Gemini failed: {e}\")\n        \n        # All providers failed\n        logger.error(\"\u274c All LLM providers failed\")\n        raise Exception(\"All AI providers are currently unavailable. Please try again later.\")\n    \n    async def _try_emergent_llm(self, prompt: str, session_id: str, system_message: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Try Emergent LLM with OpenAI GPT-4o\"\"\"\n        try:\n            chat = LlmChat(\n                api_key=self.emergent_key,\n                session_id=session_id,\n                system_message=system_message\n            ).with_model(\"openai\", \"gpt-4o\")\n            \n            response = await chat.send_message(UserMessage(text=prompt))\n            \n            if response:\n                code = self._extract_code(response)\n                return {\n                    'response': response,\n                    'code': code,\n                    'provider': 'Emergent LLM (OpenAI GPT-4o)'\n                }\n        except Exception as e:\n            logger.error(f\"Emergent LLM error: {e}\")\n            return None\n    \n    async def _try_groq(self, prompt: str, system_message: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Try Groq AI with Llama\"\"\"\n        try:\n            # Use Groq's fastest model\n            response = self.groq_client.chat.completions.create(\n                model=\"llama-3.1-70b-versatile\",  # Fast and powerful\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.7,\n                max_tokens=4000\n            )\n            \n            ai_response = response.choices[0].message.content\n            \n            if ai_response:\n                code = self._extract_code(ai_response)\n                return {\n                    'response': ai_response,\n                    'code': code,\n                    'provider': 'Groq AI (Llama 3.1 70B)'\n                }\n        except Exception as e:\n            logger.error(f\"Groq error: {e}\")\n            return None\n    \n    async def _try_gemini(self, prompt: str, system_message: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Try Google Gemini Direct API\"\"\"\n        try:\n            model = genai.GenerativeModel('gemini-2.0-flash-exp')\n            \n            full_prompt = f\"{system_message}\\n\\nUser Request: {prompt}\"\n            \n            response = model.generate_content(full_prompt)\n            ai_response = response.text\n            \n            if ai_response:\n                code = self._extract_code(ai_response)\n                return {\n                    'response': ai_response,\n                    'code': code,\n                    'provider': 'Google Gemini 2.0 Flash'\n                }\n        except Exception as e:\n            logger.error(f\"Gemini error: {e}\")\n            return None\n    \n    def _extract_code(self, response: str) -> str:\n        \"\"\"Extract HTML code from AI response\"\"\"\n        if \"```html\" in response:\n            return response.split(\"```html\")[1].split(\"```\")[0].strip()\n        elif \"```\" in response:\n            # Try to find any code block\n            code_block = response.split(\"```\")[1].split(\"```\")[0].strip()\n            # Remove language identifier if present\n            if code_block.startswith(('html', 'HTML')):\n                code_block = '\\n'.join(code_block.split('\\n')[1:])\n            return code_block\n        else:\n            # Return full response if no code blocks\n            return response\n\n# Singleton instance\n_doveable_service = None\n\ndef get_doveable_service() -> DoveableAIService:\n    \"\"\"Get or create Doveable AI service instance\"\"\"\n    global _doveable_service\n    if _doveable_service is None:\n        _doveable_service = DoveableAIService()\n    return _doveable_service"}]}, "type": "mcp_tool"}
Observation: Successfully wrote 3 files:
- /app/backend/.env
- /app/backend/services/__init__.py
- /app/backend/services/llm_service.py